\documentclass[12pt,a4paper]{article}
\usepackage[margin=2cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{fancyhdr}
\usepackage{mdframed}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{diagbox}
\usepackage{makecell}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{113-1 Machine Learning}
\fancyhead[R]{Final Project}
\fancyfoot[C]{\thepage}
\setlength{\headheight}{15pt}

\usepackage{silence}
\WarningFilter{latexfont}{Font shape}
\WarningFilter{latexfont}{Some font shapes were not available}

\usepackage{xeCJK}
\setCJKmainfont[AutoFakeBold=true, AutoFakeSlant=true]{bkai00mp.ttf}
\setCJKmonofont[AutoFakeBold=true, AutoFakeSlant=true]{bkai00mp.ttf}
\setCJKsansfont[AutoFakeBold=true, AutoFakeSlant=true]{bkai00mp.ttf}

% remove all figures for faster compilation
\usepackage[allfiguresdraft]{draftfigure}

\setlength\parindent{0pt}
\linespread{1.25}

\begin{document}
\begin{center}
  {\LARGE \bf HTML 2024 Final Project}\\[8pt]
  \textbf{Team:} How To Make Lasagna\\
  B12902119 胡祐誠 B12902123 常洧丞 B12902037 張聲元 B11902139 洪佑全
\end{center}


\section{Introduction}

\section{Preprocessing}

\section{Model}

\subsection{Logistic Regression}

Logistic regression is the only approach we use that is in the MLF. We expect that it will be the least accurate one, but we still want to see how it performs. We used the preprocessed data to train the model, and split the data into 80\% training and 20\% validation. We trained the model 5000 times and choose the one with the highest validation accuracy for both stage 1 and stage 2. The highest validation accuracy we got in stage 1 was 0.64, and in stage 2 was 0.61. In spite of this, the performance of logistic regression in private was 0.56 in stage 1, and 0.53 in stage 2. Overall, logistic regression is one of the worst models we used in this project.

\subsection{Support Vector Machine}

\subsection{Random Forest}

\subsection{Neural Network}

\subsection{AdaBoost}

\subsubsection{Decision Stumps}
Starting from the weakest base model, Decision Stumps. We experiment on how many decision stumps we need by 5-fold validation.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.49\textwidth]{images/Ada_Deci_Stump_5fold_recover.png}
  \includegraphics[width=0.49\textwidth]{images/Ada_Deci_Stump_5fold_large.png}
  \caption{Accuracy of AdaBoost with Decision Stumps}
  \label{fig:ada-deci-stump-acc}
\end{figure}
We take the best one with 100 decision stumps and get the following accuracy.
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|}
  \hline
  Sample & In sample & Validation & Stage 1 Public & Stage 1 Private & Stage 2 Public & Stage 2 Private \\ \hline
  Accuracy (\%) & 61.70 & 61.48 & 54.68 & 55.36 & 51.50 & 50.98 \\ \hline
  \end{tabular}
  \caption{Accuracy of AdaBoost with Decision Stumps }
  \label{tab:ada-deci-stump-acc}
\end{table}

Not very ideal, but we can do better with AdaBoost.

\subsubsection{Decision Trees}
AdaBoost can be coupled with better base learner such as shallow Decision Trees, which has way more hyperparameters to tune than Decision Stumps. To find the best model, we apply extensive Grid Search with 5-fold validation on these hyperparameters in Scikit-Learn's DecisionTreeClassifier and AdaBoostClassifier API.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
hyperparameter & max\_depth & max\_features & min\_samples\_leaf & min\_samples\_split & n\_estimators \\ \hline
range & {[}2,10{]} & \{None, sqrt\} & {[}1, 100{]} & {[}2, 100{]} & \multicolumn{1}{c|}{{[}20, 200{]}} \\ \hline
\end{tabular}
\caption{Hyperparameters for AdaBoost with Decision Trees }
\label{tab:my-table}
\end{table}
Each of them means
\begin{enumerate}
    \item max\_depth: The maximum depth of the decision trees.
    \item max\_features: The number of features to consider when looking for the best split. None means taking all $n$ features and sqrt means taking only $\sqrt{n}$ features. 
    \item min\_samples\_leaf: The minimum number of samples required to be at a leaf node.
    \item min\_samples\_split: The minimum number of samples required to split an internal node.
    \item n\_estimators: The number of decision trees. 
\end{enumerate}

Best $E_{val}$ happens around $max\_depth = 2$ or $=3$, max\_features being None, $min\_samples\_leaf=10$, $min\_samples\_split=2$ and $n\_estimators = 100$. We have the following accuracy.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|}
  \hline
  Sample & In sample & Validation & Stage 1 Public & Stage 1 Private & Stage 2 Public & Stage 2 Private \\ \hline
  Accuracy (\%) & 61.95 & 62.15 & 56.16 & 55.98 & 53.65 & 53.19 \\ \hline
  \end{tabular}
  \caption{Accuracy of AdaBoost with Decision Trees }
  \label{tab:ada-deci-tree-acc}
\end{table}

We can see the improvement, but we haven't unlocked the full potential of boosting yet.

\subsection{XGBoost}
XGBoost is known as eXtreme Gradient Boosting, one of the most powerful and efficient boosting model. Most interestingly, XGBoost can be trained on raw data with missing entries, so we can compare performance on all types of data. Let's see whether it can outperform AdaBoost. 

\subsubsection{Decision Trees}
XGBoost with Decision Trees has hyperparameters similar to AdaBoost's, what's interesting is that we can apply L1/L2 regularization on it. In addition, we trained XGBoost on four kinds of data, data with and without recovery, and data with and without PCA. After extensive Grid Search with 10-fold validation, we obtain the following accuracy.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  \diagbox{Data}{Sample} & In sample & Validation & \makecell{Stage 1\\ Public} & \makecell{Stage 1\\ Private} & \makecell{Stage 2 \\Public} & \makecell{Stage 2\\ Private} \\ \hline
  \makecell{No PCA, recover} & 63.65 & 56.41 & 58.16 & 58.08 & 57.72 & 54.08 \\ \hline
  \makecell{No PCA, has recover} & 61.29 & 56.18 & 57.32 & 57.82 & 53.57 & 52.21 \\ \hline
  \makecell{Has PCA, recover} & 64.94 & 62.06 & 56.29 & 56.23 & 52.65 & 52.36 \\ \hline
  \end{tabular}
  \caption{Accuracy of XGBoost with Decision Trees, With or Without PCA and Recovery }
  \label{tab:xgb-deci-tree-acc}
\end{table}

\subsubsection{Dart}

\subsubsection{Linear Regression}

\subsection{Blending}

\section{Result}

\section{Discussion}

\section{Reference}

\end{document}
